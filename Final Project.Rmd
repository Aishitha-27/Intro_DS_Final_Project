---
title: "Understanding Sales patterns and return dynamics in E-Commerce"
author: "Team 3"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r basic_libraries, include=FALSE}
library(ezids)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(outliers)
library(reshape2) 
library(lubridate)
library(scales)
library(rpart)
library(rpart.plot)
library(caret)
library(magrittr)
library(Metrics)
library(rattle)
library(corrplot)
library(pROC)
```

# Introduction

Shopping online has become a part of everyday life for many of us. From the convenience of browsing products from the comfort of our homes to having them delivered to our doorsteps, e-commerce has revolutionized how we shop. But what about the patterns behind these purchases, or the reasons why some products end up being returned? To better understand these dynamics, our team decided to dive into this fascinating topic.

By analyzing sales and return data from an e-commerce platform, we aim to uncover trends that drive sales performance and influence product returns. This analysis will help us better understand customer behavior, product preferences, and how businesses can optimize their strategies to reduce returns while enhancing the shopping experience.


## Source of the Dataset

The dataset used for this analysis is sourced from Kaggle. The variables in the dataset include:

Variable |  Definition  
  :-:    |  :-- 
Invoice number| A unique identifier for each sales transaction.
stock code| The code representing the product stock-keeping unit.
description| A brief description of the product.
quantity| The number of units of the product sold in the transaction
invoice date| The date and time when the sale was recorded.
unit price| The price per unit of the product in the transaction currency.
customer id| A unique identifier for each customer
country| The customer's country.
warehouse location| The warehouse location from which the order was fulfilled.
Shipment Provider|  The provider responsible for delivering the order.
Order Priority| The priority level of the order.
Discount| The discount applied to the transaction, if any.
Shipping cost|  The cost of shipping for the transaction.
Sales channel|  The channel through which the sale was made .

## SMART Framework Questions

The SMART framework was popularized by George T. Doran in a 1981 paper in Management Review. And we use this framework in our analysis to set goals that are clear, attainable, and meaningful.

**Specific:** The objective should be clear and state who will do what 

**Measurable:** The objective should include how the action will be measured 

**Achievable:** The objective should be realistic and attainable 

**Relevant:** The objective should make sense and fit the purpose 

**Time-bound:** The objective should include a timeline for expected results **[2]**

We have leverage the understanding of what a SMART question is and poured it into the following questions to be answered as part of our analysis;

**1)** What distinct customer segments can be identified by examining purchase frequency, product preferences, and payment methods, and how can these segments enhance personalized marketing efforts? 

**2)**Which products exhibit the highest and most consistent demand, and how do factors like discounts, order priority, or customer demographics influence this demand over time? 

**3)**How do variables like shipping costs, preferred shipment providers, and warehouse locations impact delivery times, cost-efficiency, and customer satisfaction? 

**5)**What are the performance differences between online and in-store sales in terms of sales volume, customer satisfaction, and cost efficiency, and how do these inform channel-specific growth opportunities?

**6)**What are the primary patterns in product returns (by product type, discount status, customer segment), and how can these insights refine return policies to increase customer satisfaction and reduce return rates?


## Exploratory Data Analysis 

In the scholarly realm of data analysis, Exploratory Data Analysis (EDA) stands as a pivotal phase, akin to the preliminary investigations conducted by a seasoned researcher before embarking on a comprehensive study. Picture this phase as the initial survey of an unexplored scientific landscape, where the analyst endeavors to unearth latent patterns, identify anomalies, test hypotheses, and validate assumptions through a meticulous examination of summary statistics and graphical representations.

Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.**[3]**

The main purpose of EDA is to help look at data before making any assumptions. It can help identify obvious errors, as well as better understand patterns within the data, detect outliers or anomalous events, find interesting relations among the variables.**[4]**

Moreover, EDA validates assumptions, a process analogous to literature review and theoretical grounding in academic research. It ensures that the initial assumptions made about the dataset withstand scholarly scrutiny, providing a robust foundation for subsequent analyses.

For this project, we kicked things off by getting familiar with the dataset. We took a good look at the sales data by country, explored how returns were distributed, and identified the top-selling products. We also noticed some interesting trends, like how sales varied over time and which payment methods were most popular. 

## Glimpse of a dataset
```{r}
data=read.csv("online_sales_dataset.csv")
head(data, 5)
```

## Structure and Summary of Dataset
```{r}
str(data)
```

```{r}
xkablesummary(data, title = "Summary of Dataset")
```

# Cleaning Dataset 

## Looking For Null Values
```{r}
colSums(is.na(data))
```

## Removing Null Values
```{r}
data_clean<- na.omit(data)
```

```{r}
colSums(is.na(data_clean))
```

```{r}
dim(data_clean)
```

## Random Sample
```{r}
set.seed(123) 
data_reduced <- data_clean %>% sample_n(10000)
```

```{r}
dim(data_reduced)
```

# Detecting and Removing Outliers

An outlier is a data point that significantly deviates from the majority of other data points in a dataset, and in Exploratory Data Analysis (EDA), outliers can significantly affect the interpretation of data by skewing visualizations, impacting statistical measures like the mean, and potentially misleading conclusions drawn from the analysis if not properly identified and handled. Outliers can be unusually high or low compared to the majority of the data points.**[5]**

In this analysis, we specifically focus on identifying outliers in the age groups of officers and subjects. By examining these outliers, we aim to ensure that our findings are robust and representative of the overall dataset, minimizing any undue influence from extreme values. This process strengthens the validity of our analysis by providing a more accurate portrayal of the data trends and relationships.

```{r}
ggplot(data_reduced, aes(x = "", y = Quantity)) +
    geom_boxplot(fill = "lightblue", color = "darkblue") +
    labs(title = "Boxplot of Quantity", y = "Quantity") +
    theme_minimal()
```

```{r}
ggplot(data_reduced, aes(x = "", y = UnitPrice)) +
    geom_boxplot(fill = "lightblue", color = "darkblue") +
    labs(title = "Boxplot of UnitPrice", y = "Unit Price") +
    theme_minimal()
```

```{r}
ggplot(data_reduced, aes(x = "", y = Discount)) +
    geom_boxplot(fill = "lightblue", color = "darkblue") +
    labs(title = "Boxplot of Discount", y = "Discount") +
    theme_minimal()
```

```{r}
kd2_outliers <- function(column) {
  density <- density(column, na.rm = TRUE)
  threshold <- 0.01  # Define a threshold for low-density regions
  outlier_indices <- which(density(column)$y < threshold)
  return(length(outlier_indices))  # Return the count of outliers
}

outliers_quantity <- kd2_outliers(data_reduced$Quantity)
outliers_unit_price <- kd2_outliers(data_reduced$UnitPrice)
outliers_discount <- kd2_outliers(data_reduced$Discount)

cat("Outliers in Quantity:", outliers_quantity, "\n")
cat("Outliers in Unit Price:", outliers_unit_price, "\n")
cat("Outliers in Discount:", outliers_discount, "\n")
```

**'outlierKD2'** has identified and removed outliers, but it did not actually drop the rows from the dataframe. The outlierKD2 function only returns a modified column but does not update the dataframe in place. We manually remove the rows that contain outliers from cleaned_data dataframe.

```{r}
remove_kd2_outliers <- function(df, column) {
  density <- density(df[[column]], na.rm = TRUE)
  threshold <- 0.01  # Define a threshold for low-density regions
  outlier_indices <- which(density$y < threshold)
  if (length(outlier_indices) > 0) {
    df <- df[-outlier_indices, ]
  }
  return(df)
}

cleaned_data <- data_reduced %>%
  remove_kd2_outliers("Quantity") %>%
  remove_kd2_outliers("UnitPrice") %>%
  remove_kd2_outliers("Discount")
```

## Outliers are Removed

Peak into the cleaned dataset after removing outliers

```{r}
xkablesummary(cleaned_data)
```

Converting variables into factors
```{r}
cleaned_data$InvoiceDate <- as.Date(cleaned_data$InvoiceDate, format = "%d-%m-%Y")
cleaned_data$Description = as.factor(cleaned_data$Description)
cleaned_data$StockCode = as.factor(cleaned_data$StockCode)
cleaned_data$Country = as.factor(cleaned_data$Country)
cleaned_data$PaymentMethod = as.factor(cleaned_data$PaymentMethod)
cleaned_data$Category = as.factor(cleaned_data$Category)
cleaned_data$ReturnStatus = as.factor(cleaned_data$ReturnStatus)
cleaned_data$SalesChannel = as.factor(cleaned_data$SalesChannel)
cleaned_data$ShipmentProvider = as.factor(cleaned_data$ShipmentProvider)
cleaned_data$OrderPriority = as.factor(cleaned_data$OrderPriority)
cleaned_data$WarehouseLocation = as.factor(cleaned_data$WarehouseLocation)
```

```{r}
str(cleaned_data)
```

# Vizualisation

As part of our analysis, we first pre-processed the data to ensure its integrity and completeness, establishing a reliable foundation for subsequent analysis and visualization. With a clean and well-structured dataset, we are now prepared to delve into data visualization and exploration to uncover meaningful patterns and insights.

Data visualization is a crucial element of Exploratory Data Analysis (EDA), offering a way for data analysts to engage visually with the data, which aids in understanding the relationships between variables and identifying potential trends. By creating visual representations, we could effectively interpret the dataset’s underlying structure, facilitating a more intuitive and insightful analysis.**[6]**


```{r}
cleaned_data$TotalSales <- cleaned_data$UnitPrice * cleaned_data$Quantity

sales_by_country <- cleaned_data %>%
  group_by(Country) %>%
  summarise(TotalSales = sum(TotalSales, na.rm = TRUE))

# Create the bar plot
ggplot(sales_by_country, aes(x = reorder(Country, TotalSales), y = TotalSales)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Total Sales by Country",
    x = "Country",
    y = "Total Sales"
  ) +
  theme_light()
```

```{r}
return_status_counts <- cleaned_data %>%
  group_by(ReturnStatus) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = (Count / sum(Count)) * 100)  

ggplot(return_status_counts, aes(x = "", y = Count, fill = ReturnStatus)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), position = position_stack(vjust = 0.5)) +  
  labs(
    title = "Proportion of Return Statuses",
    x = NULL,
    y = NULL
  ) +
  theme_void() +
  theme(legend.title = element_blank())
```

```{r}
top_products <- cleaned_data %>%
  group_by(Description) %>%  
  summarise(TotalSales = sum(UnitPrice * Quantity, na.rm = TRUE)) %>%  
  arrange(desc(TotalSales)) %>%  
  slice(1:10)  

ggplot(top_products, aes(x = reorder(Description, TotalSales), y = TotalSales)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  
  labs(
    title = "Top 10 Products by Total Sales",
    x = "Product Description",
    y = "Total Sales"
  ) +
  theme_minimal()
```

```{r}
cleaned_data <- cleaned_data %>%
  mutate(Year = format(InvoiceDate, "%Y")) 

category_sales_yearly <- cleaned_data %>%
  group_by(Year, Category) %>%
  summarise(
    TotalSales = sum(UnitPrice * Quantity, na.rm = TRUE), 
    .groups = "drop"
  )

category_sales_yearly$Year <- factor(category_sales_yearly$Year, levels = unique(category_sales_yearly$Year))

ggplot(category_sales_yearly, aes(x = Year, y = TotalSales, fill = Category)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    title = "Stacked Sales by Category Over Time (Yearly)",
    x = "Year",
    y = "Total Sales",
    fill = "Category"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = comma) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}

# Summarize total quantities for returned and not-returned items
sales_return_summary <- cleaned_data %>%
  group_by(Description, ReturnStatus) %>%
  summarise(
    TotalQuantity = sum(Quantity, na.rm = TRUE),
    .groups = "drop"
  )
ggplot(sales_return_summary, aes(x = reorder(Description, TotalQuantity), y = TotalQuantity, fill = ReturnStatus)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    title = "Most Returned and Not Returned Items",
    x = "Item Description",
    y = "Total Quantity",
    fill = "Return Status"
  ) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```

```{r}
product_preferences <- cleaned_data %>%
  group_by(Description, Country) %>%
  summarise(TotalSales = sum(UnitPrice * Quantity, na.rm = TRUE), .groups = "drop")

ggplot(product_preferences, aes(x = Description, y = TotalSales, fill = Country)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    title = "Product Preferences by Customer Type",
    x = "Product",
    y = "Total Sales",
    fill = "Customer Type") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"))
```

# Hypothesis Testing

In an analysis, a hypothesis is a preliminary assumption or prediction based on existing knowledge, which guides exploration and testing. This testing is typically done through statistical methods such as t-tests for mean comparisons or chi-square tests for categorical data relationships, yielding p-values that indicate whether observed patterns are statistically significant.**[7]**

Hypotheses serve as focal points in EDA, helping to frame specific questions and streamline investigation by directing attention toward the most relevant variables and relationships. Through hypothesis testing, assumptions are rigorously evaluated, allowing for meaningful interpretation and robust insights that align with the project’s objectives. By narrowing the focus, hypotheses provide structure and efficiency to EDA, connecting exploratory analysis with deeper inferential statistics or predictive modeling. This approach ensures that findings are not only descriptive but also actionable, enhancing decision-making and potentially shaping strategic direction.

## Hypothsis 1

**Null Hypothesis (H₀)**: There is no significant association between the mode of payment and customer segmentation based on purchase frequency.

**Alternative Hypothesis (H₁)**: There is a significant association between the mode of payment and customer segmentation based on purchase frequency.

**Mode of Test:** Chi-square test for independence

```{r}
unique(cleaned_data$PaymentMethod)
```

```{r}
payment_segregation <- table(cleaned_data$PaymentMethod, cleaned_data$Category)
chisq_test <- chisq.test(payment_segregation)
print(chisq_test)
```
The **p-value** is **greater** than **0.05**, hence we **fail to reject the null hypothesis** and this suggests that there is no significant association between mode of payment and customer segmentation based on purchase frequency. Based on the result we do not have enough evidence to say that there is an association between the 2 parameters.

## Hypothesis 2

**Null Hypothesis (H₀)**: Discounts have no significant impact on the average quantity purchased.

**Alternative Hypothesis (H₁)**: Discounts have a significant impact on the average quantity purchased.

**Mode of Test:** Independent samples t-test

```{r}
t_test <- t.test(Quantity ~ Discount > 0.1, data = cleaned_data)  
print(t_test)
```
The **p-value** is **greater** than **0.05**, hence we **fail to reject the null hypothesis** and this suggests that discount has no significant impact on average quantity purchased. The confidence interval further indicates that there's no strong evidence for a significant difference in the average quantity purchased with and without a discount.

## Hypothesis 3

**Null Hypothesis (H₀)**: The average shipping cost does not vary significantly between shipment providers.

**Alternative Hypothesis (H₁)**: The average shipping cost varies significantly between shipment providers.

**Mode of Test:** One-way ANOVA

```{r}
anova <- aov(ShippingCost ~ ShipmentProvider, data = cleaned_data)
summary(anova)
```
The **p-value** is **greater** than **0.05**, hence we **fail to reject the null hypothesis** and this suggests that there is no significant difference in the average shipping costs between the different shipment providers. And the low F-value is also suggesting that the variation between shipment providers is small compared to the variation within them. In conclusion, the type of shipment provider doesn’t have a strong impact on the cost of shipping.

## Hypothesis 4

**Null Hypothesis (H₀)**: There is no significant difference in sales volume between online and in-store channels.

**Alternative Hypothesis (H₁)**: Sales volume differs significantly between online and in-store channels.

**Mode of Test:** Independent samples t-test

```{r}
t_test2 <- t.test(Quantity ~ SalesChannel, data = cleaned_data)
print(t_test2)
```
The **p-value** is **greater** than **0.05**, which means we **fail to reject null hypothesis** and this suggests that there is a statistically no significant difference in sales volume between online and in-store channels. The confidence interval is further supporting the conclusion that there is no significant difference in sales volume between the two channels.

## Hypothesis 5

**Null Hypothesis (H₀)**: Return rates are not significantly higher for any particular product category.

**Alternative Hypothesis (H₁)**: Return rates are significantly higher for certain product categories.

**Mode of Test:** Chi-square test for independence

```{r}
return_category <- table(cleaned_data$ReturnStatus, cleaned_data$Category)
chisq_test2 <- chisq.test(return_category)
print(chisq_test2)
```
The **p-value** is **greater** than **0.05**, hence we **fail to reject the null hypothesis** and this suggests that there is no significant difference in return rates between the product categories. Based on the result we do not have enough evidence to say that there is an association between the 2 parameters.

## Note

**Failing to reject the null hypothesis doesn't mean the null hypothesis is true—it just means you don’t have enough evidence to reject it.**

# Models

## Model 1 - Linear Regression and Regression Tree

**What factors influence total sales revenue, and how can we predict it based on product characteristics, customer demographics and order details?**

We build **Linear Regression** to understand the significant impact of several potential explanatory variables like discounts, quantity sold, order priority, payment methods, and customer region on total sales revenue.

```{r}
sales_data <- cleaned_data %>%
  group_by(StockCode, OrderPriority, CustomerID) %>%
  summarise(
    avg_discount = mean(Discount, na.rm = TRUE),
    total_sales_revenue = sum(UnitPrice * Quantity, na.rm = TRUE),
    total_quantity_sold = sum(Quantity, na.rm = TRUE),
    .groups = "drop"
  )

customer_data <- cleaned_data %>%
  select(CustomerID, Country, PaymentMethod) %>%
  distinct(CustomerID, .keep_all = TRUE)

sales_data <- sales_data %>%
  left_join(customer_data, by = "CustomerID")
```

```{r}
cor_matrix <- cor(sales_data %>% select(avg_discount, total_sales_revenue, total_quantity_sold))

corrplot(
  cor_matrix, 
  method = "color",  
  type = "upper",    
  addCoef.col = "black",  
  tl.col = "black",  
  col = colorRampPalette(c("red", "white", "purple"))(200))
```

The heatmap reveals that **total quantity sold** has a strong positive correlation (0.65) with **total sales revenue**, indicating it is a key driver of revenue. In contrast, **average discount** shows **negligible correlation** with **total quantity sold** and **no correlation** with **total sales revenue**, suggesting discounts may not significantly impact sales performance. This highlights the need to focus on other strategies to optimize revenue.

We split the data into train and test set where we set 70% of data to be randomly selected as train data and the rest as test data.
```{r}
set.seed(123)
train_indices1 <- sample(1:nrow(sales_data), 0.7 * nrow(sales_data))
train_data1 <- sales_data[train_indices1, ]
test_data1 <- sales_data[-train_indices1, ]
```

After splitting the data into training and testing sets, we trained the model using avg_discount, OrderPriority, Country, PaymentMethod and total_quantity_sold as predictors of total_sales_revenue. 
```{r}
lm_sales_model <- lm(
  total_sales_revenue ~ avg_discount + OrderPriority + Country + PaymentMethod + total_quantity_sold,
  data = train_data1)

summary(lm_sales_model)
```
We evaluate to asses how well our linear regression model is performing. 

```{r}
fitted_values <- lm_sales_model$fitted.values
residuals <- train_data1$total_sales_revenue - fitted_values

MAE <- mean(abs(residuals))
RMSE <- sqrt(mean(residuals^2))

SS_total <- sum((train_data1$total_sales_revenue - mean(train_data1$total_sales_revenue))^2)
SS_residual <- sum(residuals^2)
R_squared <- 1 - (SS_residual / SS_total)

n <- nrow(train_data1)  
p <- length(coef(lm_sales_model)) - 1  
Adjusted_R_squared <- 1 - ((1 - R_squared) * (n - 1)) / (n - p - 1)

cat("MAE:", MAE, "\n")
cat("RMSE:", RMSE, "\n")
cat("R-squared:", R_squared, "\n")
cat("Adjusted R-squared:", Adjusted_R_squared, "\n")
```
The model's evaluation metrics suggest **moderate performance** in predicting total sales revenue. The **R-squared** value of 0.422 indicates that approximately **42% of the variance** in sales revenue is explained by the model, which means there is still considerable unexplained variability. The **Adjusted R-squared** value of 0.421 slightly adjusts for the number of predictors, indicating that the model’s explanatory power is similar even when accounting for the number of features. The **MAE** of 618.35 suggests that, on average, the model's predictions deviate from actual sales revenue by **618.35 units**. The **RMSE** of 823.68 indicates that larger errors are more significant, with the model often underestimating or overestimating revenue by larger amounts. Overall, while the model provides some useful insights, its prediction accuracy could be improved, possibly by incorporating additional relevant features or exploring alternative modeling techniques.

Since our target variable is total_sales_revenue, which is a continuous numerical variable, we also build a **Regression Tree** to explore non-linear relationships in the data. 

```{r}
regression_tree <- rpart(
  total_sales_revenue ~ avg_discount + OrderPriority + Country + PaymentMethod + total_quantity_sold,data = train_data1,method="anova")
summary(regression_tree)
```

```{r}
importance_values <- as.data.frame(regression_tree$variable.importance)

var_importance <- data.frame(
  Variable = rownames(importance_values),
  Overall = as.numeric(importance_values$`regression_tree$variable.importance`)
)

rownames(var_importance) <- NULL

ggplot(var_importance, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Feature Importance",x = "Features",
    y = "Importance")
```

The feature importance plot shows that **total_quantity_sold** is the most significant predictor in the regression tree model, dominating the influence on the target variable. Other features, such as **Country** and **avg_discount**, have comparatively much lower importance, indicating their minimal contribution to the model's predictive power.

```{r}
fancyRpartPlot(regression_tree)
```

The regression tree **predicts outcomes** based on the feature **total_quantity_sold**. The root node splits at total_quantity_sold < 25, with further splits refining the predictions based on smaller thresholds. Each leaf node represents the predicted value and its corresponding percentage of data, showing how sales are grouped based on total_quantity_sold.

```{r}
residuals_tree <- test_data1$total_sales_revenue - predict(regression_tree, test_data1)

MAE_tree <- mean(abs(residuals_tree))
RMSE_tree <- sqrt(mean(residuals_tree^2))

SS_total_tree <- sum((test_data1$total_sales_revenue - mean(test_data1$total_sales_revenue))^2)
SS_residual_tree <- sum(residuals_tree^2)
R_squared_tree <- 1 - (SS_residual_tree / SS_total_tree)

n <- nrow(test_data1)  # number of data points
p <- length(coef(regression_tree))  # number of predictors
Adjusted_R_squared_tree <- 1 - ((1 - R_squared_tree) * (n - 1)) / (n - p - 1)

cat("MAE (Tree):", MAE_tree, "\n")
cat("RMSE (Tree):", RMSE_tree, "\n")
cat("R-squared (Tree):", R_squared_tree, "\n")
cat("Adjusted R-squared (Tree):", Adjusted_R_squared_tree, "\n")
```
The model's evaluation metrics suggest **moderate performance** in predicting total sales revenue. The **R-squared** value of 0.415 indicates that approximately **41.5% of the variance** in sales revenue is explained by the regression tree model, leaving a significant portion of variability unexplained. The **Adjusted R-squared** value of 0.415 is consistent with the R-squared, indicating that the model’s explanatory power remains similar even after adjusting for the number of predictors. The **MAE** of 631.92 suggests that, on average, the model’s predictions deviate from actual sales revenue by **631.92 units**. The **RMSE** of 830.03 shows that larger prediction errors are more significant, with the model tending to make larger underestimation or overestimation. Overall, while the model provides some insight into the factors driving sales revenue, its prediction accuracy can still be improved by adding more relevant features or trying other modeling approaches.

## Model 2 - Classification Tree and Logistic Regression

**Which order characteristics significantly influence the likelihood of a sale occurring online versus in-store, and how can we use these factors to classify sales channels?**

We build **Classification Tree** model to classify customers into predefined segments based on predictors like purchase frequency, product preferences, and payment methods.

```{r}
customer_data <- cleaned_data %>%
  group_by(CustomerID) %>%
  summarize(
    purchase_frequency = n_distinct(InvoiceNo),
    total_quantity = sum(Quantity, na.rm = TRUE),
    total_spend = sum(UnitPrice * Quantity, na.rm = TRUE),
    avg_discount = mean(Discount, na.rm = TRUE),
    preferred_payment = as.character(names(sort(table(PaymentMethod), decreasing = TRUE)[1])),
    preferred_category = as.character(names(sort(table(Category), decreasing = TRUE)[1]))
  )

spend_summary <- summary(customer_data$total_spend)
low_threshold <- spend_summary["1st Qu."] 
high_threshold <- spend_summary["3rd Qu."] 

customer_data <- customer_data %>%
  mutate(
    customer_segment = case_when(
      total_spend < low_threshold ~ "Low",
      total_spend >= low_threshold & total_spend < high_threshold ~ "Medium",
      total_spend >= high_threshold ~ "High"
    )
  )

customer_data$preferred_payment <-  as.factor(customer_data$preferred_payment)

customer_data$preferred_category <- as.factor(customer_data$preferred_category)


customer_data$preferred_payment <- as.numeric(customer_data$preferred_payment)

customer_data$preferred_category <- as.numeric(customer_data$preferred_category)
```


```{r}
cor_matrix <- cor(customer_data %>% 
                    select(purchase_frequency, total_quantity, avg_discount, preferred_payment, preferred_category), 
                  use = "complete.obs")

corrplot(
  cor_matrix, 
  method = "color",  
  type = "upper",    
  addCoef.col = "black",  
  tl.col = "black",  
  col = colorRampPalette(c("red", "white", "purple"))(200),
 mar = c(0, 0, 1, 0))
```

The heatmap shows that **purchase_frequency** and **total_quantity** have a moderate positive correlation, indicating frequent purchasers tend to buy more items. Other variables, such as **avg_discount**, **preferred_payment**, and **preferred_category**, show weak or no linear correlations with others. This suggests **purchase_frequency** and **total_quantity** might be the most relevant predictors, while other features could contribute in nonlinear ways.

We aggregate features by grouping CustomerID. Then we define the customer_segment variable based on total_spend and categorized customers into "Low," "Medium," and "High" segments. Next we convert and encode preferred_payment and preferred_category to numeric form for model compatibility. We split the data into train and test set where we set 70% of data to be randomly selected as train data and the rest as test data. Then we apply the classification tree model.

```{r}
set.seed(123)
train_indices <- sample(1:nrow(customer_data), 0.7 * nrow(customer_data))
train_data <- customer_data[train_indices, ]
test_data <- customer_data[-train_indices, ]


classification_tree <- rpart(
  customer_segment ~ purchase_frequency + total_quantity + avg_discount + preferred_payment + preferred_category,
  data = train_data,
  method = "class"
)

summary(classification_tree) 
```

```{r}
fancyRpartPlot(classification_tree)
```

The model performs best for **High** segment customers, with a high accuracy of **95.7%**, indicating that total_quantity is the strongest predictor for high-value customers. For **Medium** segment customers, the accuracy is still strong at **88.8%**, while the model has **80.8%** accuracy when predicting **Low** segment customers.

```{R}
predictions <- predict(classification_tree, test_data, type = "class")
test_data$predicted_segment <- predictions
```

```{R}
confusion_matrix <- table(Actual = test_data$customer_segment, Predicted = test_data$predicted_segment)
print(confusion_matrix)
```

This confusion matrix evaluates a classification model's performance. The diagonal values represent correct predictions, with the model performing best for the **Medium class** (1009 correct). Significant confusion exists between **High** and **Medium classes**, with many misclassifications (e.g., 304 High predicted as Medium). Improving feature representation or using a different model could enhance accuracy.

```{R}
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
```

The classification tree model achieved an accuracy of **63.97%**, meaning it correctly predicted approximately 64% of the instances in the dataset. While this shows moderate performance, the model struggles with misclassifications, as seen in the confusion matrix, particularly between `High` and `Medium` categories. Further tuning, feature engineering, or trying alternative models may improve this result.

```{r}
predicted_probs <- predict(classification_tree, test_data, type = "prob")

actual <- ifelse(test_data$customer_segment == "High", 1, 0)  
predicted <- predicted_probs[, "High"] 

thresholds <- seq(0, 1, by = 0.01)
tpr <- c()  
fpr <- c()  

for (threshold in thresholds) {
  predicted_class <- ifelse(predicted >= threshold, 1, 0)
  
  tp <- sum(predicted_class == 1 & actual == 1)
  fp <- sum(predicted_class == 1 & actual == 0)
  fn <- sum(predicted_class == 0 & actual == 1)
  tn <- sum(predicted_class == 0 & actual == 0)
  
  tpr <- c(tpr, tp / (tp + fn)) 
  fpr <- c(fpr, fp / (fp + tn))
}

plot(fpr, tpr, type = "l", col = "blue", lwd = 2, xlab = "False Positive Rate", ylab = "True Positive Rate",
     main = "ROC Curve")
abline(a = 0, b = 1, col = "red", lty = 2) 
```
```{r}
sortedindices = order(fpr)
fpr = fpr[sortedindices]
tpr = tpr[sortedindices]

auc = sum((fpr[-1] - fpr[-length(fpr)]) * (tpr[-1] + tpr[-length(tpr)]) / 2)

cat("AUC:",auc, "\n")
```

The **ROC curve** evaluates the model's ability to classify "High" customer segments, with the blue line showing the trade-off between True Positive Rate (sensitivity) and False Positive Rate. The curve's proximity to the top-left corner indicates good classification performance, significantly better than the random classifier (red dashed line). A higher AUC (Area Under the Curve) reflects better overall model accuracy in distinguishing "High" from non-"High" customers.

Next we build **Logistic Regression** model to predict whether a sale occurs online or in-store, focusing on order characteristics. To prepare the dataset, we encoded the sales channel as a binary variable (SalesChannel), where “Online” was represented as 1 and “In-store” as 0. Key predictors like quantity, unit price, discount, order priority, country, and payment method were selected, ensuring that categorical variables were converted to factors for compatibility with the model.

```{r}

logistic_data <- cleaned_data %>%
  mutate(
    SalesChannel = ifelse(SalesChannel == "Online", 1, 0)  
  ) %>%
  select(
    SalesChannel, CustomerID, Quantity, UnitPrice, Discount, OrderPriority, Country, PaymentMethod
  )
```

We split the data into training and testing sets, with 70% of the data used for training. The model was then trained using Quantity, UnitPrice, Discount, OrderPriority, Country, and PaymentMethod as predictors. 

```{r}
set.seed(123)
train_indices <- sample(1:nrow(logistic_data), 0.7 * nrow(logistic_data))
train_data <- logistic_data[train_indices, ]
test_data <- logistic_data[-train_indices, ]


logistic_model <- glm(
  SalesChannel ~ Quantity + UnitPrice + Discount + OrderPriority + Country + PaymentMethod,
  data = train_data,
  family = "binomial"
)

summary(logistic_model)
```
From the model output, we find that OrderPriority is the only statistically significant predictor with **p-value = 0.0348**, which is **less** than the typical significance level of 0.05. This indicates that **OrderPriority** plays a **key role** in determining whether a sale occurs online or in-store. Other factors, such as Quantity, UnitPrice, Discount, Country, and PaymentMethod, were not found to be significant predictors in this case, as their p-values are greater than 0.05.

```{R}
predicted_probabilities <- predict(logistic_model, test_data, type = "response")
test_data$predicted_channel <- ifelse(predicted_probabilities > 0.5, 1, 0)
```

```{R}
confusion_matrix <- table(Actual = test_data$SalesChannel, Predicted = test_data$predicted_channel)
print(confusion_matrix)
```

```{r}
performance <- confusionMatrix(factor(test_data$predicted_channel), factor(test_data$SalesChannel))

accuracy <- performance$overall["Accuracy"]
print(accuracy)
```

The logistic regression model's accuracy is **49.86%**, meaning it correctly predicts the target class in about half of the cases. This suggests the model is only slightly better than random guessing for a multi-class classification problem. To improve performance, consider feature engineering, balancing the dataset, or experimenting with other models like decision trees or ensemble methods.

```{r}
predicted_probs = predict(logistic_model, test_data, type = "response")

roc_curve= roc(test_data$SalesChannel, predicted_probabilities)

plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve")
abline(a = 0, b = 1, col = "red", lty = 2)
```

```{r}
auc_value = auc(roc_curve)
cat("AUC:", auc_value, "\n")
```








